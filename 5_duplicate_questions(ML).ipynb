{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1y2kz2xRF8h-",
    "outputId": "57c2d4df-a443-4b34-bcd8-38f783684e4a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (1.13.1)\n",
      "Collecting scipy\n",
      "  Downloading scipy-1.15.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.0/62.0 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy<2.5,>=1.23.5 in /usr/local/lib/python3.11/dist-packages (from scipy) (1.24.3)\n",
      "Downloading scipy-1.15.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (37.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m37.7/37.7 MB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: scipy\n",
      "  Attempting uninstall: scipy\n",
      "    Found existing installation: scipy 1.13.1\n",
      "    Uninstalling scipy-1.13.1:\n",
      "      Successfully uninstalled scipy-1.13.1\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "gensim 4.3.3 requires scipy<1.14.0,>=1.7.0, but you have scipy 1.15.3 which is incompatible.\n",
      "pymc 5.23.0 requires numpy>=1.25.0, but you have numpy 1.24.3 which is incompatible.\n",
      "jaxlib 0.5.1 requires numpy>=1.25, but you have numpy 1.24.3 which is incompatible.\n",
      "xarray-einstats 0.9.0 requires numpy>=1.25, but you have numpy 1.24.3 which is incompatible.\n",
      "jax 0.5.2 requires numpy>=1.25, but you have numpy 1.24.3 which is incompatible.\n",
      "albumentations 2.0.8 requires numpy>=1.24.4, but you have numpy 1.24.3 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed scipy-1.15.3\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade scipy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ga76WtgJGDYP",
    "outputId": "c67ccec9-b6a2-4fb0-a631-d2e67ca55e9f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting numpy==1.26.4\n",
      "  Using cached numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
      "Using cached numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n",
      "Installing collected packages: numpy\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.24.3\n",
      "    Uninstalling numpy-1.24.3:\n",
      "      Successfully uninstalled numpy-1.24.3\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "gensim 4.3.3 requires scipy<1.14.0,>=1.7.0, but you have scipy 1.15.3 which is incompatible.\n",
      "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed numpy-1.26.4\n"
     ]
    }
   ],
   "source": [
    "!pip install numpy==1.26.4 --force-reinstall\n",
    "import os\n",
    "os.kill(os.getpid(), 9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "n2CwlUMKFCMf"
   },
   "outputs": [],
   "source": [
    "!pip install nltk distance fuzzywuzzy gensim --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lq3h6RAJm-Bc"
   },
   "outputs": [],
   "source": [
    "!pip install distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9-X-hbp2nKXw"
   },
   "outputs": [],
   "source": [
    "!pip install fuzzywuzzy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1qMjTFlBnQiL"
   },
   "outputs": [],
   "source": [
    "!pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e9REyZh5njis"
   },
   "outputs": [],
   "source": [
    "!pip install numpy==1.24.3 --force-reinstall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jqKyiTMLnqwz"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.kill(os.getpid(), 9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pR986LsmFqqM",
    "outputId": "601e6fdf-72fc-472d-d945-25bdeeec2d58"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\Lib\\site-packages\\fuzzywuzzy\\fuzz.py:11: UserWarning: Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning\n",
      "  warnings.warn('Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning')\n",
      "D:\\Anaconda\\Lib\\site-packages\\paramiko\\pkey.py:82: CryptographyDeprecationWarning: TripleDES has been moved to cryptography.hazmat.decrepit.ciphers.algorithms.TripleDES and will be removed from this module in 48.0.0.\n",
      "  \"cipher\": algorithms.TripleDES,\n",
      "D:\\Anaconda\\Lib\\site-packages\\paramiko\\transport.py:219: CryptographyDeprecationWarning: Blowfish has been moved to cryptography.hazmat.decrepit.ciphers.algorithms.Blowfish and will be removed from this module in 45.0.0.\n",
      "  \"class\": algorithms.Blowfish,\n",
      "D:\\Anaconda\\Lib\\site-packages\\paramiko\\transport.py:243: CryptographyDeprecationWarning: TripleDES has been moved to cryptography.hazmat.decrepit.ciphers.algorithms.TripleDES and will be removed from this module in 48.0.0.\n",
      "  \"class\": algorithms.TripleDES,\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "import distance\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from fuzzywuzzy import fuzz\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dvHWK0K5Fs_Y",
    "outputId": "b8b647bd-ecd4-44f4-8093-4002d7ca5e59"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\KUNAL\n",
      "[nltk_data]     PUNIA\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to C:\\Users\\KUNAL\n",
      "[nltk_data]     PUNIA\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "tpxBcCARGXvk"
   },
   "outputs": [],
   "source": [
    "df_data = pd.read_csv(r\"D:\\Data Science\\NLP\\Day7_Duplicate_question_pair\\questions.csv\")\n",
    "df_data.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "i-MHWB0SdAN4"
   },
   "outputs": [],
   "source": [
    "# Shuffle and select 50% of the dataset\n",
    "df = df_data.sample(frac=0.5, random_state=42).reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7wiIAhzRdI5r",
    "outputId": "7f65b683-f8e9-4d93-e7a9-306db05bd7ab"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(202174, 6)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423
    },
    "id": "hhN5R2d3GbqP",
    "outputId": "faa55d87-d520-4c2c-c188-a45a5fcf1eac"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>qid1</th>\n",
       "      <th>qid2</th>\n",
       "      <th>question1</th>\n",
       "      <th>question2</th>\n",
       "      <th>is_duplicate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>371032</td>\n",
       "      <td>726051</td>\n",
       "      <td>726052</td>\n",
       "      <td>Do people realize that you can send marijuana ...</td>\n",
       "      <td>How do you send weed through the mail?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>263498</td>\n",
       "      <td>518455</td>\n",
       "      <td>518456</td>\n",
       "      <td>How can rock music be brought back?</td>\n",
       "      <td>What would it take for rock music to make a co...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>111008</td>\n",
       "      <td>220125</td>\n",
       "      <td>220126</td>\n",
       "      <td>Why does one feel relaxed after smoking a join...</td>\n",
       "      <td>How do I sober up quickly after smoking weed/m...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>75665</td>\n",
       "      <td>150351</td>\n",
       "      <td>150352</td>\n",
       "      <td>How to gain weight ?</td>\n",
       "      <td>How do I gain weight fast but still be healthy?</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>52376</td>\n",
       "      <td>104229</td>\n",
       "      <td>104230</td>\n",
       "      <td>Is porn bad for men?</td>\n",
       "      <td>Can I become a porn fan without getting addicted?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202169</th>\n",
       "      <td>356935</td>\n",
       "      <td>699008</td>\n",
       "      <td>699009</td>\n",
       "      <td>What advantages does Google Home have over Ama...</td>\n",
       "      <td>In what ways is Google Home (Assistant) smarte...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202170</th>\n",
       "      <td>224366</td>\n",
       "      <td>442219</td>\n",
       "      <td>442220</td>\n",
       "      <td>In-app Billing: Is there a limit to the number...</td>\n",
       "      <td>Why is my app InApp Purchase not showing?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202171</th>\n",
       "      <td>367711</td>\n",
       "      <td>719673</td>\n",
       "      <td>719674</td>\n",
       "      <td>Why is it so that a very few officers make it ...</td>\n",
       "      <td>Why is it so that a very few officers make it ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202172</th>\n",
       "      <td>91833</td>\n",
       "      <td>182331</td>\n",
       "      <td>182332</td>\n",
       "      <td>What's your favourite scene from a TV series?</td>\n",
       "      <td>What is your favourite scene from a TV show?</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202173</th>\n",
       "      <td>260336</td>\n",
       "      <td>512331</td>\n",
       "      <td>512332</td>\n",
       "      <td>Is an open relationship a better user experien...</td>\n",
       "      <td>What are some NGOs in India working for stray ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>202174 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            id    qid1    qid2  \\\n",
       "0       371032  726051  726052   \n",
       "1       263498  518455  518456   \n",
       "2       111008  220125  220126   \n",
       "3        75665  150351  150352   \n",
       "4        52376  104229  104230   \n",
       "...        ...     ...     ...   \n",
       "202169  356935  699008  699009   \n",
       "202170  224366  442219  442220   \n",
       "202171  367711  719673  719674   \n",
       "202172   91833  182331  182332   \n",
       "202173  260336  512331  512332   \n",
       "\n",
       "                                                question1  \\\n",
       "0       Do people realize that you can send marijuana ...   \n",
       "1                     How can rock music be brought back?   \n",
       "2       Why does one feel relaxed after smoking a join...   \n",
       "3                                    How to gain weight ?   \n",
       "4                                    Is porn bad for men?   \n",
       "...                                                   ...   \n",
       "202169  What advantages does Google Home have over Ama...   \n",
       "202170  In-app Billing: Is there a limit to the number...   \n",
       "202171  Why is it so that a very few officers make it ...   \n",
       "202172      What's your favourite scene from a TV series?   \n",
       "202173  Is an open relationship a better user experien...   \n",
       "\n",
       "                                                question2  is_duplicate  \n",
       "0                  How do you send weed through the mail?             0  \n",
       "1       What would it take for rock music to make a co...             1  \n",
       "2       How do I sober up quickly after smoking weed/m...             0  \n",
       "3         How do I gain weight fast but still be healthy?             1  \n",
       "4       Can I become a porn fan without getting addicted?             0  \n",
       "...                                                   ...           ...  \n",
       "202169  In what ways is Google Home (Assistant) smarte...             1  \n",
       "202170          Why is my app InApp Purchase not showing?             0  \n",
       "202171  Why is it so that a very few officers make it ...             1  \n",
       "202172       What is your favourite scene from a TV show?             1  \n",
       "202173  What are some NGOs in India working for stray ...             0  \n",
       "\n",
       "[202174 rows x 6 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "67bNzd6xGc6w"
   },
   "outputs": [],
   "source": [
    "# Preprocessing Function\n",
    "stemmer = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "O1Cf6h8EGiL9"
   },
   "outputs": [],
   "source": [
    "def preprocess(q):\n",
    "    q = str(q).lower().strip()\n",
    "\n",
    "    q = q.replace('%', ' percent').replace('$', ' dollar ').replace('₹', ' rupee ')\n",
    "    q = q.replace('€', ' euro ').replace('@', ' at ').replace('[math]', '')\n",
    "\n",
    "    q = re.sub(r'([0-9]+)000000000', r'\\\\1b', q)\n",
    "    q = re.sub(r'([0-9]+)000000', r'\\\\1m', q)\n",
    "    q = re.sub(r'([0-9]+)000', r'\\\\1k', q)\n",
    "\n",
    "    # Decontract\n",
    "    contractions = {\n",
    "    \"ain't\": \"am not\",\n",
    "    \"aren't\": \"are not\",\n",
    "    \"can't\": \"can not\",\n",
    "    \"can't've\": \"can not have\",\n",
    "    \"'cause\": \"because\",\n",
    "    \"could've\": \"could have\",\n",
    "    \"couldn't\": \"could not\",\n",
    "    \"couldn't've\": \"could not have\",\n",
    "    \"didn't\": \"did not\",\n",
    "    \"doesn't\": \"does not\",\n",
    "    \"don't\": \"do not\",\n",
    "    \"hadn't\": \"had not\",\n",
    "    \"hadn't've\": \"had not have\",\n",
    "    \"hasn't\": \"has not\",\n",
    "    \"haven't\": \"have not\",\n",
    "    \"he'd\": \"he would\",\n",
    "    \"he'd've\": \"he would have\",\n",
    "    \"he'll\": \"he will\",\n",
    "    \"he'll've\": \"he will have\",\n",
    "    \"he's\": \"he is\",\n",
    "    \"how'd\": \"how did\",\n",
    "    \"how'd'y\": \"how do you\",\n",
    "    \"how'll\": \"how will\",\n",
    "    \"how's\": \"how is\",\n",
    "    \"i'd\": \"i would\",\n",
    "    \"i'd've\": \"i would have\",\n",
    "    \"i'll\": \"i will\",\n",
    "    \"i'll've\": \"i will have\",\n",
    "    \"i'm\": \"i am\",\n",
    "    \"i've\": \"i have\",\n",
    "    \"isn't\": \"is not\",\n",
    "    \"it'd\": \"it would\",\n",
    "    \"it'd've\": \"it would have\",\n",
    "    \"it'll\": \"it will\",\n",
    "    \"it'll've\": \"it will have\",\n",
    "    \"it's\": \"it is\",\n",
    "    \"let's\": \"let us\",\n",
    "    \"ma'am\": \"madam\",\n",
    "    \"mayn't\": \"may not\",\n",
    "    \"might've\": \"might have\",\n",
    "    \"mightn't\": \"might not\",\n",
    "    \"mightn't've\": \"might not have\",\n",
    "    \"must've\": \"must have\",\n",
    "    \"mustn't\": \"must not\",\n",
    "    \"mustn't've\": \"must not have\",\n",
    "    \"needn't\": \"need not\",\n",
    "    \"needn't've\": \"need not have\",\n",
    "    \"o'clock\": \"of the clock\",\n",
    "    \"oughtn't\": \"ought not\",\n",
    "    \"oughtn't've\": \"ought not have\",\n",
    "    \"shan't\": \"shall not\",\n",
    "    \"sha'n't\": \"shall not\",\n",
    "    \"shan't've\": \"shall not have\",\n",
    "    \"she'd\": \"she would\",\n",
    "    \"she'd've\": \"she would have\",\n",
    "    \"she'll\": \"she will\",\n",
    "    \"she'll've\": \"she will have\",\n",
    "    \"she's\": \"she is\",\n",
    "    \"should've\": \"should have\",\n",
    "    \"shouldn't\": \"should not\",\n",
    "    \"shouldn't've\": \"should not have\",\n",
    "    \"so've\": \"so have\",\n",
    "    \"so's\": \"so as\",\n",
    "    \"that'd\": \"that would\",\n",
    "    \"that'd've\": \"that would have\",\n",
    "    \"that's\": \"that is\",\n",
    "    \"there'd\": \"there would\",\n",
    "    \"there'd've\": \"there would have\",\n",
    "    \"there's\": \"there is\",\n",
    "    \"they'd\": \"they would\",\n",
    "    \"they'd've\": \"they would have\",\n",
    "    \"they'll\": \"they will\",\n",
    "    \"they'll've\": \"they will have\",\n",
    "    \"they're\": \"they are\",\n",
    "    \"they've\": \"they have\",\n",
    "    \"to've\": \"to have\",\n",
    "    \"wasn't\": \"was not\",\n",
    "    \"we'd\": \"we would\",\n",
    "    \"we'd've\": \"we would have\",\n",
    "    \"we'll\": \"we will\",\n",
    "    \"we'll've\": \"we will have\",\n",
    "    \"we're\": \"we are\",\n",
    "    \"we've\": \"we have\",\n",
    "    \"weren't\": \"were not\",\n",
    "    \"what'll\": \"what will\",\n",
    "    \"what'll've\": \"what will have\",\n",
    "    \"what're\": \"what are\",\n",
    "    \"what's\": \"what is\",\n",
    "    \"what've\": \"what have\",\n",
    "    \"when's\": \"when is\",\n",
    "    \"when've\": \"when have\",\n",
    "    \"where'd\": \"where did\",\n",
    "    \"where's\": \"where is\",\n",
    "    \"where've\": \"where have\",\n",
    "    \"who'll\": \"who will\",\n",
    "    \"who'll've\": \"who will have\",\n",
    "    \"who's\": \"who is\",\n",
    "    \"who've\": \"who have\",\n",
    "    \"why's\": \"why is\",\n",
    "    \"why've\": \"why have\",\n",
    "    \"will've\": \"will have\",\n",
    "    \"won't\": \"will not\",\n",
    "    \"won't've\": \"will not have\",\n",
    "    \"would've\": \"would have\",\n",
    "    \"wouldn't\": \"would not\",\n",
    "    \"wouldn't've\": \"would not have\",\n",
    "    \"y'all\": \"you all\",\n",
    "    \"y'all'd\": \"you all would\",\n",
    "    \"y'all'd've\": \"you all would have\",\n",
    "    \"y'all're\": \"you all are\",\n",
    "    \"y'all've\": \"you all have\",\n",
    "    \"you'd\": \"you would\",\n",
    "    \"you'd've\": \"you would have\",\n",
    "    \"you'll\": \"you will\",\n",
    "    \"you'll've\": \"you will have\",\n",
    "    \"you're\": \"you are\",\n",
    "    \"you've\": \"you have\"\n",
    "    }\n",
    "    q_tokens = q.split()\n",
    "    q_tokens = [contractions[word] if word in contractions else word for word in q_tokens]\n",
    "    q = ' '.join(q_tokens)\n",
    "\n",
    "    q = BeautifulSoup(q, 'lxml').get_text()\n",
    "    q = re.sub(r\"[^a-zA-Z0-9]\", \" \", q)\n",
    "\n",
    "    # Remove stopwords and apply stemming + lemmatization\n",
    "    tokens = q.split()\n",
    "    tokens = [\n",
    "        lemmatizer.lemmatize(stemmer.stem(word))\n",
    "        for word in tokens if word not in stop_words\n",
    "    ]\n",
    "\n",
    "    return ' '.join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "SWdfvldvGlaV"
   },
   "outputs": [],
   "source": [
    "df['question1'] = df['question1'].apply(preprocess)\n",
    "df['question2'] = df['question2'].apply(preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "BrUxAEDRGnk4"
   },
   "outputs": [],
   "source": [
    "# Advanced Features\n",
    "#token based features\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "def basic_features(row):\n",
    "\n",
    "    q1 = row['question1']\n",
    "    q2 = row['question2']\n",
    "\n",
    "    SAFE_DIV = 0.0001\n",
    "\n",
    "    STOP_WORDS = stopwords.words(\"english\")\n",
    "\n",
    "    token_features = [0.0]*8\n",
    "\n",
    "    # Converting the Sentence into Tokens:\n",
    "    q1_tokens = q1.split()\n",
    "    q2_tokens = q2.split()\n",
    "\n",
    "    if len(q1_tokens) == 0 or len(q2_tokens) == 0:\n",
    "        return token_features\n",
    "\n",
    "    # Get the non-stopwords in Questions\n",
    "    q1_words = set([word for word in q1_tokens if word not in STOP_WORDS])\n",
    "    q2_words = set([word for word in q2_tokens if word not in STOP_WORDS])\n",
    "\n",
    "    #Get the stopwords in Questions\n",
    "    q1_stops = set([word for word in q1_tokens if word in STOP_WORDS])\n",
    "    q2_stops = set([word for word in q2_tokens if word in STOP_WORDS])\n",
    "\n",
    "    # Get the common non-stopwords from Question pair\n",
    "    common_word_count = len(q1_words.intersection(q2_words))\n",
    "\n",
    "    # Get the common stopwords from Question pair\n",
    "    common_stop_count = len(q1_stops.intersection(q2_stops))\n",
    "\n",
    "    # Get the common Tokens from Question pair\n",
    "    common_token_count = len(set(q1_tokens).intersection(set(q2_tokens)))\n",
    "\n",
    "\n",
    "    token_features[0] = common_word_count / (min(len(q1_words), len(q2_words)) + SAFE_DIV)\n",
    "    token_features[1] = common_word_count / (max(len(q1_words), len(q2_words)) + SAFE_DIV)\n",
    "    token_features[2] = common_stop_count / (min(len(q1_stops), len(q2_stops)) + SAFE_DIV)\n",
    "    token_features[3] = common_stop_count / (max(len(q1_stops), len(q2_stops)) + SAFE_DIV)\n",
    "    token_features[4] = common_token_count / (min(len(q1_tokens), len(q2_tokens)) + SAFE_DIV)\n",
    "    token_features[5] = common_token_count / (max(len(q1_tokens), len(q2_tokens)) + SAFE_DIV)\n",
    "\n",
    "    # Last word of both question is same or not\n",
    "    token_features[6] = int(q1_tokens[-1] == q2_tokens[-1])\n",
    "\n",
    "    # First word of both question is same or not\n",
    "    token_features[7] = int(q1_tokens[0] == q2_tokens[0])\n",
    "\n",
    "    return token_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "MBr59Q6YGzWA"
   },
   "outputs": [],
   "source": [
    "token_features = df.apply(basic_features, axis=1)\n",
    "\n",
    "df[\"cwc_min\"]       = list(map(lambda x: x[0], token_features))\n",
    "df[\"cwc_max\"]       = list(map(lambda x: x[1], token_features))\n",
    "df[\"csc_min\"]       = list(map(lambda x: x[2], token_features))\n",
    "df[\"csc_max\"]       = list(map(lambda x: x[3], token_features))\n",
    "df[\"ctc_min\"]       = list(map(lambda x: x[4], token_features))\n",
    "df[\"ctc_max\"]       = list(map(lambda x: x[5], token_features))\n",
    "df[\"last_word_eq\"]  = list(map(lambda x: x[6], token_features))\n",
    "df[\"first_word_eq\"] = list(map(lambda x: x[7], token_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 521
    },
    "id": "47XH1ExhHb_o",
    "outputId": "939f482e-3d22-4a8e-f12f-12594502ee01"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>qid1</th>\n",
       "      <th>qid2</th>\n",
       "      <th>question1</th>\n",
       "      <th>question2</th>\n",
       "      <th>is_duplicate</th>\n",
       "      <th>cwc_min</th>\n",
       "      <th>cwc_max</th>\n",
       "      <th>csc_min</th>\n",
       "      <th>csc_max</th>\n",
       "      <th>ctc_min</th>\n",
       "      <th>ctc_max</th>\n",
       "      <th>last_word_eq</th>\n",
       "      <th>first_word_eq</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>371032</td>\n",
       "      <td>726051</td>\n",
       "      <td>726052</td>\n",
       "      <td>peopl realiz send marijuana overnight carrier ...</td>\n",
       "      <td>send weed mail</td>\n",
       "      <td>0</td>\n",
       "      <td>0.333322</td>\n",
       "      <td>0.124998</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.333322</td>\n",
       "      <td>0.124998</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>263498</td>\n",
       "      <td>518455</td>\n",
       "      <td>518456</td>\n",
       "      <td>rock music brought back</td>\n",
       "      <td>would take rock music make come back</td>\n",
       "      <td>1</td>\n",
       "      <td>0.749981</td>\n",
       "      <td>0.428565</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.749981</td>\n",
       "      <td>0.428565</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>111008</td>\n",
       "      <td>220125</td>\n",
       "      <td>220126</td>\n",
       "      <td>one feel relax smoke joint marijuana</td>\n",
       "      <td>sober quickli smoke weed marijuana drug</td>\n",
       "      <td>0</td>\n",
       "      <td>0.333328</td>\n",
       "      <td>0.333328</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.333328</td>\n",
       "      <td>0.333328</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>75665</td>\n",
       "      <td>150351</td>\n",
       "      <td>150352</td>\n",
       "      <td>gain weight</td>\n",
       "      <td>gain weight fast still healthi</td>\n",
       "      <td>1</td>\n",
       "      <td>0.999950</td>\n",
       "      <td>0.399992</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.999950</td>\n",
       "      <td>0.399992</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>52376</td>\n",
       "      <td>104229</td>\n",
       "      <td>104230</td>\n",
       "      <td>porn bad men</td>\n",
       "      <td>becom porn fan without get addict</td>\n",
       "      <td>0</td>\n",
       "      <td>0.333322</td>\n",
       "      <td>0.166664</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.333322</td>\n",
       "      <td>0.166664</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id    qid1    qid2                                          question1  \\\n",
       "0  371032  726051  726052  peopl realiz send marijuana overnight carrier ...   \n",
       "1  263498  518455  518456                            rock music brought back   \n",
       "2  111008  220125  220126               one feel relax smoke joint marijuana   \n",
       "3   75665  150351  150352                                        gain weight   \n",
       "4   52376  104229  104230                                       porn bad men   \n",
       "\n",
       "                                 question2  is_duplicate   cwc_min   cwc_max  \\\n",
       "0                           send weed mail             0  0.333322  0.124998   \n",
       "1     would take rock music make come back             1  0.749981  0.428565   \n",
       "2  sober quickli smoke weed marijuana drug             0  0.333328  0.333328   \n",
       "3           gain weight fast still healthi             1  0.999950  0.399992   \n",
       "4        becom porn fan without get addict             0  0.333322  0.166664   \n",
       "\n",
       "   csc_min  csc_max   ctc_min   ctc_max  last_word_eq  first_word_eq  \n",
       "0      0.0      0.0  0.333322  0.124998           0.0            0.0  \n",
       "1      0.0      0.0  0.749981  0.428565           1.0            0.0  \n",
       "2      0.0      0.0  0.333328  0.333328           0.0            0.0  \n",
       "3      0.0      0.0  0.999950  0.399992           0.0            1.0  \n",
       "4      0.0      0.0  0.333322  0.166664           0.0            0.0  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "I9sA27JIHw0D"
   },
   "outputs": [],
   "source": [
    "# length based features\n",
    "import distance\n",
    "\n",
    "def fetch_length_features(row):\n",
    "\n",
    "    q1 = row['question1']\n",
    "    q2 = row['question2']\n",
    "\n",
    "    length_features = [0.0]*3\n",
    "\n",
    "    # Converting the Sentence into Tokens:\n",
    "    q1_tokens = q1.split()\n",
    "    q2_tokens = q2.split()\n",
    "\n",
    "    if len(q1_tokens) == 0 or len(q2_tokens) == 0:\n",
    "        return length_features\n",
    "\n",
    "    # Absolute length features\n",
    "    length_features[0] = abs(len(q1_tokens) - len(q2_tokens))\n",
    "\n",
    "    # Average Token Length of both Questions\n",
    "    length_features[1] = (len(q1_tokens) + len(q2_tokens)) / 2\n",
    "\n",
    "    # Longest common substring ratio\n",
    "    strs = list(distance.lcsubstrings(q1, q2))\n",
    "    if strs:\n",
    "        length_features[2] = len(strs[0]) / (min(len(q1), len(q2)) + 1)\n",
    "    else:\n",
    "        length_features[2] = 0.0  # if no common substring\n",
    "\n",
    "    return length_features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "SrgNbU4jH3tM"
   },
   "outputs": [],
   "source": [
    "length_features = df.apply(fetch_length_features, axis=1)\n",
    "\n",
    "df['abs_len_diff'] = list(map(lambda x: x[0], length_features))\n",
    "df['mean_len'] = list(map(lambda x: x[1], length_features))\n",
    "df['longest_substr_ratio'] = list(map(lambda x: x[2], length_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 521
    },
    "id": "4RHxva4GH95D",
    "outputId": "cd4ede93-4d35-4b5b-a098-c5333ad09034"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>qid1</th>\n",
       "      <th>qid2</th>\n",
       "      <th>question1</th>\n",
       "      <th>question2</th>\n",
       "      <th>is_duplicate</th>\n",
       "      <th>cwc_min</th>\n",
       "      <th>cwc_max</th>\n",
       "      <th>csc_min</th>\n",
       "      <th>csc_max</th>\n",
       "      <th>ctc_min</th>\n",
       "      <th>ctc_max</th>\n",
       "      <th>last_word_eq</th>\n",
       "      <th>first_word_eq</th>\n",
       "      <th>abs_len_diff</th>\n",
       "      <th>mean_len</th>\n",
       "      <th>longest_substr_ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>371032</td>\n",
       "      <td>726051</td>\n",
       "      <td>726052</td>\n",
       "      <td>peopl realiz send marijuana overnight carrier ...</td>\n",
       "      <td>send weed mail</td>\n",
       "      <td>0</td>\n",
       "      <td>0.333322</td>\n",
       "      <td>0.124998</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.333322</td>\n",
       "      <td>0.124998</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.5</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>263498</td>\n",
       "      <td>518455</td>\n",
       "      <td>518456</td>\n",
       "      <td>rock music brought back</td>\n",
       "      <td>would take rock music make come back</td>\n",
       "      <td>1</td>\n",
       "      <td>0.749981</td>\n",
       "      <td>0.428565</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.749981</td>\n",
       "      <td>0.428565</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.5</td>\n",
       "      <td>0.458333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>111008</td>\n",
       "      <td>220125</td>\n",
       "      <td>220126</td>\n",
       "      <td>one feel relax smoke joint marijuana</td>\n",
       "      <td>sober quickli smoke weed marijuana drug</td>\n",
       "      <td>0</td>\n",
       "      <td>0.333328</td>\n",
       "      <td>0.333328</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.333328</td>\n",
       "      <td>0.333328</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.270270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>75665</td>\n",
       "      <td>150351</td>\n",
       "      <td>150352</td>\n",
       "      <td>gain weight</td>\n",
       "      <td>gain weight fast still healthi</td>\n",
       "      <td>1</td>\n",
       "      <td>0.999950</td>\n",
       "      <td>0.399992</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.999950</td>\n",
       "      <td>0.399992</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>0.916667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>52376</td>\n",
       "      <td>104229</td>\n",
       "      <td>104230</td>\n",
       "      <td>porn bad men</td>\n",
       "      <td>becom porn fan without get addict</td>\n",
       "      <td>0</td>\n",
       "      <td>0.333322</td>\n",
       "      <td>0.166664</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.333322</td>\n",
       "      <td>0.166664</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>0.384615</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id    qid1    qid2                                          question1  \\\n",
       "0  371032  726051  726052  peopl realiz send marijuana overnight carrier ...   \n",
       "1  263498  518455  518456                            rock music brought back   \n",
       "2  111008  220125  220126               one feel relax smoke joint marijuana   \n",
       "3   75665  150351  150352                                        gain weight   \n",
       "4   52376  104229  104230                                       porn bad men   \n",
       "\n",
       "                                 question2  is_duplicate   cwc_min   cwc_max  \\\n",
       "0                           send weed mail             0  0.333322  0.124998   \n",
       "1     would take rock music make come back             1  0.749981  0.428565   \n",
       "2  sober quickli smoke weed marijuana drug             0  0.333328  0.333328   \n",
       "3           gain weight fast still healthi             1  0.999950  0.399992   \n",
       "4        becom porn fan without get addict             0  0.333322  0.166664   \n",
       "\n",
       "   csc_min  csc_max   ctc_min   ctc_max  last_word_eq  first_word_eq  \\\n",
       "0      0.0      0.0  0.333322  0.124998           0.0            0.0   \n",
       "1      0.0      0.0  0.749981  0.428565           1.0            0.0   \n",
       "2      0.0      0.0  0.333328  0.333328           0.0            0.0   \n",
       "3      0.0      0.0  0.999950  0.399992           0.0            1.0   \n",
       "4      0.0      0.0  0.333322  0.166664           0.0            0.0   \n",
       "\n",
       "   abs_len_diff  mean_len  longest_substr_ratio  \n",
       "0           5.0       5.5              0.333333  \n",
       "1           3.0       5.5              0.458333  \n",
       "2           0.0       6.0              0.270270  \n",
       "3           3.0       3.5              0.916667  \n",
       "4           3.0       4.5              0.384615  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "-asjJOq6I1Zk"
   },
   "outputs": [],
   "source": [
    "# Fuzzy Features\n",
    "from fuzzywuzzy import fuzz\n",
    "\n",
    "def fetch_fuzzy_features(row):\n",
    "\n",
    "    q1 = row['question1']\n",
    "    q2 = row['question2']\n",
    "\n",
    "    fuzzy_features = [0.0]*4\n",
    "\n",
    "    # fuzz_ratio\n",
    "    fuzzy_features[0] = fuzz.QRatio(q1, q2)\n",
    "\n",
    "    # fuzz_partial_ratio\n",
    "    fuzzy_features[1] = fuzz.partial_ratio(q1, q2)\n",
    "\n",
    "    # token_sort_ratio\n",
    "    fuzzy_features[2] = fuzz.token_sort_ratio(q1, q2)\n",
    "\n",
    "    # token_set_ratio\n",
    "    fuzzy_features[3] = fuzz.token_set_ratio(q1, q2)\n",
    "\n",
    "    return fuzzy_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "Eevo6iflJLsM"
   },
   "outputs": [],
   "source": [
    "fuzzy_features = df.apply(fetch_fuzzy_features, axis=1)\n",
    "\n",
    "# Creating new feature columns for fuzzy features\n",
    "df['fuzz_ratio'] = list(map(lambda x: x[0], fuzzy_features))\n",
    "df['fuzz_partial_ratio'] = list(map(lambda x: x[1], fuzzy_features))\n",
    "df['token_sort_ratio'] = list(map(lambda x: x[2], fuzzy_features))\n",
    "df['token_set_ratio'] = list(map(lambda x: x[3], fuzzy_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 568
    },
    "id": "3v01GQ5oJSPL",
    "outputId": "55a21759-068f-4443-ca32-226649f4c2f4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(202174, 21)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>qid1</th>\n",
       "      <th>qid2</th>\n",
       "      <th>question1</th>\n",
       "      <th>question2</th>\n",
       "      <th>is_duplicate</th>\n",
       "      <th>cwc_min</th>\n",
       "      <th>cwc_max</th>\n",
       "      <th>csc_min</th>\n",
       "      <th>csc_max</th>\n",
       "      <th>...</th>\n",
       "      <th>ctc_max</th>\n",
       "      <th>last_word_eq</th>\n",
       "      <th>first_word_eq</th>\n",
       "      <th>abs_len_diff</th>\n",
       "      <th>mean_len</th>\n",
       "      <th>longest_substr_ratio</th>\n",
       "      <th>fuzz_ratio</th>\n",
       "      <th>fuzz_partial_ratio</th>\n",
       "      <th>token_sort_ratio</th>\n",
       "      <th>token_set_ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>371032</td>\n",
       "      <td>726051</td>\n",
       "      <td>726052</td>\n",
       "      <td>peopl realiz send marijuana overnight carrier ...</td>\n",
       "      <td>send weed mail</td>\n",
       "      <td>0</td>\n",
       "      <td>0.333322</td>\n",
       "      <td>0.124998</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.124998</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.5</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>23</td>\n",
       "      <td>57</td>\n",
       "      <td>31</td>\n",
       "      <td>44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>263498</td>\n",
       "      <td>518455</td>\n",
       "      <td>518456</td>\n",
       "      <td>rock music brought back</td>\n",
       "      <td>would take rock music make come back</td>\n",
       "      <td>1</td>\n",
       "      <td>0.749981</td>\n",
       "      <td>0.428565</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.428565</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.5</td>\n",
       "      <td>0.458333</td>\n",
       "      <td>58</td>\n",
       "      <td>65</td>\n",
       "      <td>58</td>\n",
       "      <td>79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>111008</td>\n",
       "      <td>220125</td>\n",
       "      <td>220126</td>\n",
       "      <td>one feel relax smoke joint marijuana</td>\n",
       "      <td>sober quickli smoke weed marijuana drug</td>\n",
       "      <td>0</td>\n",
       "      <td>0.333328</td>\n",
       "      <td>0.333328</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.333328</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.270270</td>\n",
       "      <td>56</td>\n",
       "      <td>58</td>\n",
       "      <td>48</td>\n",
       "      <td>59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>75665</td>\n",
       "      <td>150351</td>\n",
       "      <td>150352</td>\n",
       "      <td>gain weight</td>\n",
       "      <td>gain weight fast still healthi</td>\n",
       "      <td>1</td>\n",
       "      <td>0.999950</td>\n",
       "      <td>0.399992</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.399992</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>0.916667</td>\n",
       "      <td>54</td>\n",
       "      <td>100</td>\n",
       "      <td>54</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>52376</td>\n",
       "      <td>104229</td>\n",
       "      <td>104230</td>\n",
       "      <td>porn bad men</td>\n",
       "      <td>becom porn fan without get addict</td>\n",
       "      <td>0</td>\n",
       "      <td>0.333322</td>\n",
       "      <td>0.166664</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.166664</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>0.384615</td>\n",
       "      <td>31</td>\n",
       "      <td>58</td>\n",
       "      <td>44</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       id    qid1    qid2                                          question1  \\\n",
       "0  371032  726051  726052  peopl realiz send marijuana overnight carrier ...   \n",
       "1  263498  518455  518456                            rock music brought back   \n",
       "2  111008  220125  220126               one feel relax smoke joint marijuana   \n",
       "3   75665  150351  150352                                        gain weight   \n",
       "4   52376  104229  104230                                       porn bad men   \n",
       "\n",
       "                                 question2  is_duplicate   cwc_min   cwc_max  \\\n",
       "0                           send weed mail             0  0.333322  0.124998   \n",
       "1     would take rock music make come back             1  0.749981  0.428565   \n",
       "2  sober quickli smoke weed marijuana drug             0  0.333328  0.333328   \n",
       "3           gain weight fast still healthi             1  0.999950  0.399992   \n",
       "4        becom porn fan without get addict             0  0.333322  0.166664   \n",
       "\n",
       "   csc_min  csc_max  ...   ctc_max  last_word_eq  first_word_eq  abs_len_diff  \\\n",
       "0      0.0      0.0  ...  0.124998           0.0            0.0           5.0   \n",
       "1      0.0      0.0  ...  0.428565           1.0            0.0           3.0   \n",
       "2      0.0      0.0  ...  0.333328           0.0            0.0           0.0   \n",
       "3      0.0      0.0  ...  0.399992           0.0            1.0           3.0   \n",
       "4      0.0      0.0  ...  0.166664           0.0            0.0           3.0   \n",
       "\n",
       "   mean_len  longest_substr_ratio  fuzz_ratio  fuzz_partial_ratio  \\\n",
       "0       5.5              0.333333          23                  57   \n",
       "1       5.5              0.458333          58                  65   \n",
       "2       6.0              0.270270          56                  58   \n",
       "3       3.5              0.916667          54                 100   \n",
       "4       4.5              0.384615          31                  58   \n",
       "\n",
       "   token_sort_ratio  token_set_ratio  \n",
       "0                31               44  \n",
       "1                58               79  \n",
       "2                48               59  \n",
       "3                54              100  \n",
       "4                44               50  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "bjMH37M_JVlU"
   },
   "outputs": [],
   "source": [
    "# TF-IDF Feature Extraction\n",
    "all_questions = pd.Series(df['question1'].tolist() + df['question2'].tolist())\n",
    "tfidf = TfidfVectorizer(max_features=900)\n",
    "tfidf.fit(all_questions)\n",
    "q1_tfidf = tfidf.transform(df['question1'])\n",
    "q2_tfidf = tfidf.transform(df['question2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "nTaNfye_Jmeg"
   },
   "outputs": [],
   "source": [
    "# Word2Vec\n",
    "sentences = all_questions.apply(lambda x: x.split()).tolist()\n",
    "w2v = Word2Vec(sentences, vector_size=100, window=5, min_count=1, workers=4)\n",
    "def avg_w2v(sentence):\n",
    "    words = sentence.split()\n",
    "    vector = np.zeros(100)\n",
    "    count = 0\n",
    "    for w in words:\n",
    "        if w in w2v.wv:\n",
    "            vector += w2v.wv[w]\n",
    "            count += 1\n",
    "    return vector / (count + 1e-5)\n",
    "q1_w2v = np.vstack(df['question1'].apply(avg_w2v))\n",
    "q2_w2v = np.vstack(df['question2'].apply(avg_w2v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "B9R64PABg7jZ"
   },
   "outputs": [],
   "source": [
    "# Only do this if these features are missing\n",
    "df['q1_len'] = df['question1'].str.len()\n",
    "df['q2_len'] = df['question2'].str.len()\n",
    "df['q1_words'] = df['question1'].apply(lambda x: len(str(x).split()))\n",
    "df['q2_words'] = df['question2'].apply(lambda x: len(str(x).split()))\n",
    "df['common_words'] = df.apply(lambda x: len(set(str(x['question1']).split()) & set(str(x['question2']).split())), axis=1)\n",
    "df['total_words'] = df['q1_words'] + df['q2_words']\n",
    "df['word_share'] = df['common_words'] / (df['total_words'] + 1e-5)\n",
    "\n",
    "from fuzzywuzzy import fuzz\n",
    "df['fuzz_ratio'] = df.apply(lambda x: fuzz.QRatio(str(x['question1']), str(x['question2'])), axis=1)\n",
    "df['fuzz_partial_ratio'] = df.apply(lambda x: fuzz.partial_ratio(str(x['question1']), str(x['question2'])), axis=1)\n",
    "df['token_sort_ratio'] = df.apply(lambda x: fuzz.token_sort_ratio(str(x['question1']), str(x['question2'])), axis=1)\n",
    "df['token_set_ratio'] = df.apply(lambda x: fuzz.token_set_ratio(str(x['question1']), str(x['question2'])), axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "LsaJJbu-JpAZ"
   },
   "outputs": [],
   "source": [
    "# Combine features\n",
    "# X_tfidf = np.hstack([q1_tfidf.toarray(), q2_tfidf.toarray()])\n",
    "from scipy.sparse import hstack, csr_matrix\n",
    "import numpy as np\n",
    "\n",
    "# Combine TF-IDF features\n",
    "X_tfidf = hstack([q1_tfidf, q2_tfidf])  # Already sparse\n",
    "\n",
    "# Combine Word2Vec features (dense)\n",
    "X_w2v = np.hstack([q1_w2v, q2_w2v])\n",
    "\n",
    "# Combine manual features (dense)\n",
    "X_manual = df[['q1_len', 'q2_len', 'q1_words', 'q2_words', 'common_words',\n",
    "               'total_words', 'word_share', 'fuzz_ratio', 'fuzz_partial_ratio',\n",
    "               'token_sort_ratio', 'token_set_ratio']].values\n",
    "\n",
    "# Convert dense features to sparse for compatibility\n",
    "X_manual_sparse = csr_matrix(X_manual)\n",
    "X_w2v_sparse = csr_matrix(X_w2v)\n",
    "\n",
    "# Choose one of the following:\n",
    "\n",
    "# Option 1: Manual + TF-IDF\n",
    "# X = hstack([X_manual_sparse, X_tfidf])\n",
    "\n",
    "# Option 2: Manual + Word2Vec\n",
    "# X = np.hstack([X_manual, X_w2v])\n",
    "\n",
    "# ✅ Option 3: Manual + TF-IDF + Word2Vec (best practice)\n",
    "X = hstack([X_manual_sparse, X_tfidf, X_w2v_sparse])\n",
    "\n",
    "# Target\n",
    "y = df['is_duplicate'].values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "7ulNnUqIZawh"
   },
   "outputs": [],
   "source": [
    "# Train/Test Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HqHF883HLIJp",
    "outputId": "a3472e4f-cd05-4553-a169-e9b63cfdfc42"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression Accuracy: 0.7313466056634104\n",
      "Confusion Matrix:\n",
      " [[20918  4566]\n",
      " [ 6297  8654]]\n"
     ]
    }
   ],
   "source": [
    "# Model Training\n",
    "#Logistic regression\n",
    "model = LogisticRegression(max_iter=200)\n",
    "model.fit(X_train, y_train)\n",
    "preds = model.predict(X_test)\n",
    "\n",
    "print(\"LogisticRegression Accuracy:\", accuracy_score(y_test, preds))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KAfBSSzDLLc8",
    "outputId": "f16941d0-a4bb-47f3-88f5-db1cf306ec9d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost Accuracy: 0.7937925064919006\n",
      "Confusion Matrix:\n",
      " [[21788  3696]\n",
      " [ 4642 10309]]\n"
     ]
    }
   ],
   "source": [
    "#XGBoost\n",
    "model = XGBClassifier(n_estimators=100, use_label_encoder=False, eval_metric='logloss')\n",
    "model.fit(X_train, y_train)\n",
    "preds = model.predict(X_test)\n",
    "\n",
    "print(\"XGBoost Accuracy:\", accuracy_score(y_test, preds))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FPd3_Kd4ihWS",
    "outputId": "a0f1037e-7d4e-4b16-f198-04c556b404c2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForest Accuracy: 0.7762581921602572\n",
      "Confusion Matrix:\n",
      " [[22703  2781]\n",
      " [ 6266  8685]]\n"
     ]
    }
   ],
   "source": [
    "#Random Forest\n",
    "model = RandomForestClassifier(n_estimators=10, n_jobs=-1)\n",
    "model.fit(X_train, y_train)\n",
    "preds = model.predict(X_test)\n",
    "\n",
    "print(\"RandomForest Accuracy:\", accuracy_score(y_test, preds))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ct2S4dHWihfD"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Define param grids\n",
    "param_grids = {\n",
    "    'RandomForest': {\n",
    "        'model': RandomForestClassifier(),\n",
    "        'params': {\n",
    "            'n_estimators': [20, 30],\n",
    "            'max_depth': [None, 10, 15],\n",
    "            'min_samples_split': [2, 5],\n",
    "            'min_samples_leaf': [1, 2]\n",
    "        }\n",
    "    },\n",
    "    'XGBoost': {\n",
    "        'model': XGBClassifier(eval_metric='logloss', use_label_encoder=False),\n",
    "        'params': {\n",
    "            'n_estimators': [100, 200],\n",
    "            'max_depth': [3, 6, 10],\n",
    "            'learning_rate': [0.05, 0.1],\n",
    "            'subsample': [0.8, 1.0]\n",
    "        }\n",
    "    },\n",
    "    'LogisticRegression': {\n",
    "        'model': LogisticRegression(max_iter=200),\n",
    "        'params': {\n",
    "            'C': [0.01, 0.1, 1, 10],\n",
    "            'solver': ['liblinear', 'saga']\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "best_models = {}\n",
    "\n",
    "for name, cfg in param_grids.items():\n",
    "    print(f\"🔍 Tuning {name}...\")\n",
    "    clf = RandomizedSearchCV(\n",
    "        cfg['model'], cfg['params'], cv=3, n_iter=5,\n",
    "        scoring='accuracy', n_jobs=-1, verbose=1, random_state=42\n",
    "    )\n",
    "    clf.fit(X_train, y_train)\n",
    "    best_models[name] = clf.best_estimator_\n",
    "    print(f\"✅ Best params for {name}: {clf.best_params_}\")\n",
    "\n",
    "# Evaluate all best models\n",
    "print(\"\\n📊 Final Evaluation:\")\n",
    "for name, model in best_models.items():\n",
    "    preds = model.predict(X_test)\n",
    "    acc = accuracy_score(y_test, preds)\n",
    "    print(f\"\\n🔹 {name} Accuracy: {acc:.4f}\")\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(confusion_matrix(y_test, preds))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MHtWMHVtz_0n"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
